{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle \n",
    "path = Path('./')\n",
    "train_path = path.joinpath('train.csv')\n",
    "test_path = path.joinpath('test.csv')\n",
    "submission_path = path.joinpath('submission_popular.csv')\n",
    "metadata_path = path.joinpath('item_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_meta = pd.read_csv(metadata_path)\n",
    "df_submission = pd.read_csv(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = set(df_train['action_type'].values)\n",
    "reference_list = set(df_train['reference'].values)\n",
    "platform_list = set(df_train['platform'].values)\n",
    "city_list = set(df_train['city'].values)\n",
    "device_list = set(df_train['device'].values)\n",
    "filter_list = set(df_train['current_filters'].values)\n",
    "filter_list =  [x for x in filter_list if str(x) != 'nan']\n",
    "\n",
    "action_list = list(action_list)\n",
    "action_list.sort()\n",
    "\n",
    "reference_list = list(reference_list)\n",
    "reference_list.sort()\n",
    "\n",
    "platform_list = list(platform_list)\n",
    "platform_list.sort()\n",
    "\n",
    "city_list = list(city_list)\n",
    "city_list.sort()\n",
    "\n",
    "device_list = list(device_list)\n",
    "device_list.sort()\n",
    "\n",
    "filter_list = list(filter_list)\n",
    "filter_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference embedding 보조 함수\n",
    "items = df_meta['item_id'].values\n",
    "properties = df_meta['properties'].values\n",
    "all_items_test = []\n",
    "for i in range(len(properties)):\n",
    "    p_data = properties[i].split('|')\n",
    "    all_items_test += p_data\n",
    "set_items = set(all_items_test)\n",
    "\n",
    "list_items = list(set_items)\n",
    "list_items.sort()\n",
    "def embedding_item_diction():\n",
    "    all_items = {}\n",
    "    for i in range(len(properties)):\n",
    "        all_items[items[i]] = properties[i].split('|')\n",
    "    embedding_items = {}\n",
    "    #print(len(list_items))\n",
    "    for keys in all_items.keys():\n",
    "        item_embeddings = [0]*len(list_items)\n",
    "        for i in range(len(list_items)):\n",
    "            if list_items[i] in all_items[keys]:\n",
    "                item_embeddings[i] = 1\n",
    "        embedding_items[keys] = item_embeddings\n",
    "    return embedding_items\n",
    "reference_diction = embedding_item_diction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터에서 마지막 reference가 nan인지 확인\n",
    "def check_data(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        if(isinstance(data[i][-1,5], float)):\n",
    "            if(math.isnan(data[i][-1,5]) == True):\n",
    "                pass\n",
    "            else:\n",
    "                print('error')\n",
    "                print(data[i])\n",
    "                max_error +=1\n",
    "        else:\n",
    "            print('error')\n",
    "            print(data[i])\n",
    "            max_error +=1\n",
    "        if(max_error > 10):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터에서 reference가 nan일때 그 전 action이 click out인지 확인\n",
    "def check_data_alpha(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        for j in range(len(data[i])):\n",
    "            if(isinstance(data[i][j,5], float)):\n",
    "                if(math.isnan(data[i][j,5]) == True):\n",
    "                     if(data[i][j,4] != 'clickout item'):\n",
    "                        print(data[i])\n",
    "                        max_error +=1\n",
    "        \n",
    "        if(max_error > 10):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터에서 reference에 하나라도 nan이 있는지 판별 있다면 그 중 적어도 하나는 action이 click out인지 판별\n",
    "def check_data_beta(data):\n",
    "    max_error = 0\n",
    "    datalength = len(data)\n",
    "    for i in range(datalength):\n",
    "        error_data_number = 0\n",
    "        click_check = False\n",
    "        for j in range(len(data[i])):\n",
    "            if(isinstance(data[i][j,5], float) and math.isnan(data[i][j,5]) == True):\n",
    "                error_data_number += 1\n",
    "        if(error_data_number == 0):\n",
    "            print(data[i])\n",
    "            print('error')\n",
    "            max_error +=1\n",
    "        if(error_data_number != 0):\n",
    "            for j in range(len(data[i])):\n",
    "                if(isinstance(data[i][j,5], float) and math.isnan(data[i][j,5]) == True):\n",
    "                    if(data[i][j,4] == 'clickout item'):\n",
    "                        click_check = True\n",
    "        if(click_check == False):\n",
    "            print('error')\n",
    "            print(data[i])\n",
    "            max_error +=1\n",
    "            #print(data[i])\n",
    "            #print('datas')\n",
    "        if(max_error > 10):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inference(data):\n",
    "    datalength = len(data)\n",
    "    maxlength = 0\n",
    "    correct_data = 0\n",
    "    new_data = []\n",
    "    for i in range(datalength):\n",
    "        if(data[i][-1,5] in data[i][-1,10].split('|')):\n",
    "            if int(data[i][-1,5]) in reference_diction:\n",
    "                new_data.append(data[i])\n",
    "                correct_data += 1\n",
    "        else:\n",
    "            pass\n",
    "            #print(i)\n",
    "            #print('error')\n",
    "            #maxlength +=1\n",
    "    print(correct_data)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이닝 데이터를 세션 단위로 나눔\n",
    "def make_data(data): \n",
    "    data_split = []\n",
    "    temp_data = []\n",
    "    train_size = len(data)\n",
    "    action_index = 1\n",
    "    for i in range(train_size):\n",
    "        if(i==0):\n",
    "            temp_data.append(data[i])\n",
    "        if(i>0):\n",
    "            if(data[i,action_index] == data[i-1,action_index]):\n",
    "                temp_data.append(data[i])\n",
    "            else:\n",
    "                data_split.append(np.array(temp_data))\n",
    "                temp_data = []\n",
    "                temp_data.append(data[i])\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_split = make_data(df_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트레이닝 데이터에서 click out이 존재하는 데이터만 뽑아냄\n",
    "#트레이닝 데이터에서 끝부분이 click out이 되도록 뒷부분을 자름\n",
    "def data_preprocess(data): \n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        check_data = False\n",
    "        final_click = 0\n",
    "        for j in range(len(data[i])):\n",
    "            if(data[i][j,4] =='clickout item'):\n",
    "                final_click = j\n",
    "                check_data = True\n",
    "        if(final_click < len(data[i])-1 and check_data):\n",
    "            new_data.append(np.delete(data[i],np.s_[final_click+1:],0))\n",
    "        elif(check_data):\n",
    "            new_data.append(data[i])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_preprocess = data_preprocess(train_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826874"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_preprocess) #총 데이터 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826199\n"
     ]
    }
   ],
   "source": [
    "train_data_preprocess = check_inference(train_data_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927142"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference_diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference_diction[1257342])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter embedding 보조함수\n",
    "all_filters = []\n",
    "for i in range(len(filter_list)):\n",
    "    if i>0:\n",
    "        all_filters += filter_list[i].split('|')\n",
    "all_filter_set = set(all_filters)\n",
    "all_filter_list = list(all_filter_set)\n",
    "all_filter_list.sort()\n",
    "def filter_embedding(data, version):\n",
    "    embedding_data = [0]*202\n",
    "    if version == 1:\n",
    "        datalist = data.split('|')\n",
    "        for filters in datalist:\n",
    "            embedding_data[all_filter_list.index(filters)] = 1\n",
    "        return embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(number, length):\n",
    "    return [int(i==number) for i in range(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_list(list1, list2):\n",
    "    length = len(list1)\n",
    "    final_list = []\n",
    "    for i in range(length):\n",
    "        final_list.append(list1[i] +list2[i])\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(city_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 action embedding\n",
    "def embedding(data, action_index):\n",
    "    if action_index == 0: #user_id dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 1: # session_id dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 2: # timestep dim 0\n",
    "        return []\n",
    "    \n",
    "    if action_index == 3: #step dim 1\n",
    "        return []\n",
    "    \n",
    "    if action_index == 4: #action_type dim 10\n",
    "        #print(one_hot_encoding(action_list.index(data), 10))\n",
    "        return one_hot_encoding(action_list.index(data), 10)\n",
    "    \n",
    "    if action_index == 5: #reference dim 157\n",
    "        if RepresentsInt(data):\n",
    "            return reference_diction[int(data)]\n",
    "        else:\n",
    "            return [0]*157\n",
    "        \n",
    "    if action_index == 6: #platform dim 55\n",
    "        return one_hot_encoding(platform_list.index(data), 55)\n",
    "    \n",
    "    if action_index == 7: #city dim 0\n",
    "        #return city_list.index(data)\n",
    "        return []\n",
    "    \n",
    "    if action_index == 8: #device dim 3\n",
    "        return one_hot_encoding(device_list.index(data), 3)\n",
    "    \n",
    "    if action_index == 9: #current_filters dim 202\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*202\n",
    "        else:\n",
    "            return filter_embedding(data, 1)\n",
    "        \n",
    "    if action_index == 10: #impressions dim 157\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*157\n",
    "        else:\n",
    "            impression_embedding = [0]*157\n",
    "            impression_list = data.split('|')\n",
    "            for impression in impression_list:\n",
    "                try:\n",
    "                    impression_embedding =sum_list(impression_embedding, reference_diction[int(impression)])\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "                    #print(int(impression))\n",
    "            '''\n",
    "            if(len(impression_embedding) < 3925):\n",
    "                impression_embedding += [0]*(3925-len(impression_embedding))   \n",
    "            '''\n",
    "            return impression_embedding\n",
    "            \n",
    "        \n",
    "    if action_index == 11: #prices dim 25\n",
    "        if(isinstance(data, float) and math.isnan(data) == True):\n",
    "            return [0]*25\n",
    "        else:\n",
    "            price_embedding = []\n",
    "            for prices in data.split('|'):\n",
    "                price_embedding.append(int(prices))\n",
    "            if(len(price_embedding) < 25):\n",
    "                price_embedding += [0]*(25-len(price_embedding))   \n",
    "            return price_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_last_click(data):\n",
    "    data_number = 0\n",
    "    for i in range(len(data)):\n",
    "        if(data[i][-1,4] == 'clickout item'):\n",
    "            data_number +=1\n",
    "    return data_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_type_length = 10\n",
    "reference_length = 157\n",
    "platform_length = 55\n",
    "device_length = 3\n",
    "current_filters_length = 202\n",
    "impresssions_length = 3925\n",
    "prices_length = 25\n",
    "def addVector(vector1, vector2, coef, action_num):\n",
    "    new_action_type = []\n",
    "    new_reference = []\n",
    "    new_platform = []\n",
    "    new_device = []\n",
    "    new_filters = []\n",
    "    new_impressions = []\n",
    "    new_prices = [] \n",
    "    #print(len(vector1))\n",
    "    #print(len(vector2))\n",
    "    for i in range(10):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "    \n",
    "    for i in range(10, 167):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "        \n",
    "    for i in range(167, 222):\n",
    "        new_action_type.append(vector1[i])\n",
    "    \n",
    "    for i in range(222, 225):\n",
    "        new_action_type.append(vector1[i])\n",
    "        \n",
    "    for i in range(225, 427):\n",
    "        new_action_type.append(vector1[i] + np.power(coef,action_num)*vector2[i])\n",
    "        \n",
    "    return new_action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeEmbeddingVector1(data):\n",
    "    #embedding_vector = []\n",
    "    #temp_vector = []\n",
    "    session_vector = [0]*609 # session data embedding(length 10+157+55+3+202+157+25=609)\n",
    "    item_vector = [] # item data embedding\n",
    "    num_action = len(data)\n",
    "    time_delay_coef = 0.1\n",
    "\n",
    "    for i in range(num_action-1, -1, -1): #거꾸로\n",
    "        timestep_vector = []\n",
    "        for j in range(12):\n",
    "            timestep_vector += embedding(data[i,j], j) \n",
    "        if(i == num_action-1):\n",
    "            for idx in range(10, 167):\n",
    "                timestep_vector[idx] = 0 # reference 부분을 0으로 만듦\n",
    "        session_vector = addVector(session_vector, timestep_vector, time_delay_coef, num_action-1-i) #session vector 생성   \n",
    "    item_vector.append(reference_diction[int(data[-1,5])])\n",
    "    temp_impressions = data[-1, 10].split('|')\n",
    "    for i in range(len(temp_impressions)):\n",
    "\n",
    "        try:\n",
    "            if(data[-1,5] != temp_impressions[i]):\n",
    "                item_vector.append(reference_diction[int(temp_impressions[i])])\n",
    "                #item_vector.append(int(temp_impressions[i]))\n",
    "        except:\n",
    "            #print('error')\n",
    "            pass\n",
    "    return session_vector, item_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeEmbeddingVector2(data):\n",
    "    #embedding_vector = []\n",
    "    #temp_vector = []\n",
    "    session_vector = [0]*609 # session data embedding(length 10+157+55+3+202+157+25=609)\n",
    "    item_vector = [] # item data embedding\n",
    "    num_action = len(data)\n",
    "    time_delay_coef = 0.1\n",
    "    correct_index = []\n",
    "    for i in range(num_action-1, -1, -1): #거꾸로\n",
    "        timestep_vector = []\n",
    "        for j in range(12):\n",
    "            timestep_vector += embedding(data[i,j], j) \n",
    "        if(i == num_action-1):\n",
    "            for idx in range(10, 167):\n",
    "                timestep_vector[idx] = 0 # reference 부분을 0으로 만듦\n",
    "        session_vector = addVector(session_vector, timestep_vector, time_delay_coef, num_action-1-i) #session vector 생성   \n",
    "    #item_vector.append(reference_diction[int(data[-1,5])])\n",
    "    temp_impressions = data[-1, 10].split('|')\n",
    "    for i in range(len(temp_impressions)):\n",
    "        if(data[-1,5] == temp_impressions[i]):\n",
    "            correct_index.append(i)\n",
    "        item_vector.append(reference_diction[int(temp_impressions[i])])\n",
    "    return session_vector, item_vector, correct_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeEmbeddingVector3(data):\n",
    "    #embedding_vector = []\n",
    "    #temp_vector = []\n",
    "    #session_vector = [0]*609 # session data embedding(length 10+157+55+3+202+157+25=609)\n",
    "    item_vector = [] # item data embedding\n",
    "    num_action = len(data)\n",
    "    #time_delay_coef = 0.1\n",
    "    correct_index = []\n",
    "    reference_vector = []\n",
    "    filter_vector = [0]*202\n",
    "    current_reference  = ''\n",
    "    reference_size = 0\n",
    "    for i in range(num_action-1, -1, -1): #거꾸로\n",
    "        timestep_vector = []\n",
    "        if i != num_action-1 and RepresentsInt(data[i, 5]) and current_reference != data[i,5] and reference_size < 2:\n",
    "            current_reference = data[1,5]\n",
    "            if int(data[i, 5]) in reference_diction:\n",
    "                reference_size += 1\n",
    "                reference_vector += reference_diction[int(data[i, 5])]\n",
    "        if(filter_vector == [0]*202):\n",
    "            filter_vector = embedding(data[i,9], 9)\n",
    "    reference_vector += [0]*(314-len(reference_vector))\n",
    "    session_vector = embedding(data[-1, 4], 4) + reference_vector + embedding(data[-1, 6], 6) + embedding(data[-1, 8], 8) + filter_vector\n",
    "    # 10+314+202+55+3 = 584\n",
    "    '''\n",
    "    for j in range(12):\n",
    "        timestep_vector += embedding(data[i,j], j) \n",
    "\n",
    "    if(i == num_action-1):\n",
    "        for idx in range(10, 167):\n",
    "            timestep_vector[idx] = 0 # reference 부분을 0으로 만듦\n",
    "    session_vector = addVector(session_vector, timestep_vector, time_delay_coef, num_action-1-i) #session vector 생성\n",
    "    '''\n",
    "    #item_vector.append(reference_diction[int(data[-1,5])])\n",
    "    temp_impressions = data[-1, 10].split('|')\n",
    "    for i in range(len(temp_impressions)):\n",
    "        if(data[-1,5] == temp_impressions[i]):\n",
    "            correct_index.append(i)\n",
    "        if int(temp_impressions[i]) in reference_diction:\n",
    "            item_vector.append(reference_diction[int(temp_impressions[i])])\n",
    "        else:\n",
    "            item_vector.append([0]*157)\n",
    "    return session_vector, item_vector, correct_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0,0,0]\n",
    "x[0:3] =[1,1,1]\n",
    "x + [0]*(3-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sevec, itemvec, crindex = MakeEmbeddingVector3(train_data_preprocess[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itemvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making data.... 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'temp_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-00c24c7d722d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     '''\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfinal_train_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mfinal_train_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_train' is not defined"
     ]
    }
   ],
   "source": [
    "del df_train\n",
    "del df_test\n",
    "del df_meta\n",
    "del df_submission\n",
    "del train_data_split\n",
    "\n",
    "final_train_data = []\n",
    "final_train_label = []\n",
    "datalength = len(train_data_preprocess)\n",
    "\n",
    "num_error = 0\n",
    "for i in range(datalength):\n",
    "    if(i%10000 ==0):\n",
    "        print('making data....', i)\n",
    "    try:\n",
    "        temp_train, temp_label = MakeEmbeddingVector(train_data_preprocess[i])\n",
    "    except:\n",
    "        num_error += 1\n",
    "    '''    \n",
    "    if(num_error>10):\n",
    "        break\n",
    "    '''\n",
    "    final_train_data.append(temp_train)\n",
    "    final_train_label.append(temp_label)\n",
    "    \n",
    "#final_train_data = np.asarray(final_train_data)\n",
    "#final_train_label = np.asarray(final_train_label)\n",
    "print('number of error is : ', num_error)\n",
    "print('total data is : ', len(final_train_data))\n",
    "with open('final_train_data.pickle', 'wb') as f:\n",
    "    pickle.dump(final_train_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('final_train_label.pickle', 'wb') as f:\n",
    "    pickle.dump(final_train_label, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826199"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "data_num = 826874\n",
    "train_embedding_dim= 584\n",
    "item_embedding_dim = 157\n",
    "latent_dim = 200\n",
    "learning_rate = 1e-3\n",
    "session_data = np.zeros((data_num,train_embedding_dim))\n",
    "#positive_label = np.zeros((data_num, item_embedding_dim))\n",
    "#negative_labels = np.zeros((data_num, item_embedding_dim))\n",
    "#negative_number = np.zeros((data_num, 1))\n",
    "item = np.zeros((3,4)) # 임시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session_feature = tf.get_variable(\"session_feature\", [train_embedding_dim, latent_dim], initializer=tf.random_normal_initializer())\n",
    "item_feature = tf.get_variable(\"item_feature\", [item_embedding_dim, latent_dim], initializer=tf.random_normal_initializer())\n",
    "session_bias = tf.get_variable(\"session_bias\", [train_embedding_dim], initializer=tf.random_normal_initializer())\n",
    "item_bias = tf.get_variable(\"item_bias\", [item_embedding_dim], initializer=tf.random_normal_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.placeholder(tf.float32, shape=(train_embedding_dim))\n",
    "train_items  = tf.placeholder(tf.float32, shape=(None, item_embedding_dim))\n",
    "train_index = tf.placeholder(tf.int32, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_2:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"MatMul_3:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"MatMul_4:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.matmul(tf.matmul(train_items, item_feature), tf.transpose(tf.matmul(tf.expand_dims(train_data,0), session_feature))))\n",
    "#x =  tf.matmul(tf.matmul(train_items, item_feature), tf.transpose(tf.matmul(tf.expand_dims(train_data,0), session_feature)))\n",
    "print(tf.matmul(tf.expand_dims(train_data,0), tf.expand_dims(session_bias,1)))\n",
    "print(tf.matmul(train_items, tf.expand_dims(item_bias,1)))\n",
    "\n",
    "scores = tf.matmul(tf.matmul(train_items, item_feature), tf.transpose(tf.matmul(tf.expand_dims(train_data,0), session_feature))) +  \\\n",
    "tf.matmul(tf.expand_dims(train_data,0), tf.expand_dims(session_bias,1)) +\\\n",
    "tf.matmul(train_items, tf.expand_dims(item_bias,1))\n",
    "#print(positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(tf.math.log(tf.sigmoid(scores[train_index[0]] - scores)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = tf.reduce_mean(tf.sigmoid(scores- scores[train_index[0]]) + tf.sigmoid(tf.math.pow(scores,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.cast(tf.argmax(scores)[0] == train_index[0], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_items, correct_index = MakeEmbeddingVector3(train_data_preprocess[2])\n",
    "batch_data  = np.asarray(batch_data)\n",
    "batch_items = np.asarray(batch_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sc = sess.run(scores, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "sc = sess.run(scores, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "#_loss = sess.run(scores, feed_dict = {train_data : batch_data, train_items : batch_items})\n",
    "#te = sess.run(temp_value, feed_dict = {train_data : batch_data, train_items : batch_items})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sess.run(scores, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MRS(score, idx):\n",
    "    return (list(np.squeeze(np.argsort(score, axis = 0))).index(idx)+1)/np.shape(score)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(np.squeeze(np.argsort(sc, axis = 0))).index(2)+1)/np.shape(sc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826199\n"
     ]
    }
   ],
   "source": [
    "all_data_number = len(train_data_preprocess)\n",
    "print(all_data_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session saved\n",
      "step 1\n",
      "output index is :  13\n",
      "correct index is :  0\n",
      "corrent rs is :  0.92\n",
      "train mean loss is :  1.1587109565734863\n",
      "valid mean loss is :  1.5023402222394944\n",
      "train mean reciprocal score is :  0.8600000000000001\n",
      "valid mean reciprocal score is :  0.5243914744565799\n",
      "data num is :  2\n",
      "session saved\n",
      "step 1001\n",
      "output index is :  13\n",
      "correct index is :  18\n",
      "corrent rs is :  0.22727272727272727\n",
      "train mean loss is :  1.4834808979034424\n",
      "valid mean loss is :  1.4716440079808235\n",
      "train mean reciprocal score is :  0.5421881856402826\n",
      "valid mean reciprocal score is :  0.5538091584858805\n",
      "data num is :  1000\n",
      "step 2001\n",
      "output index is :  14\n",
      "correct index is :  1\n",
      "corrent rs is :  0.5\n",
      "train mean loss is :  1.4618776369690896\n",
      "valid mean loss is :  1.4736577596664429\n",
      "train mean reciprocal score is :  0.5643460027414817\n",
      "valid mean reciprocal score is :  0.5529836259952711\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 3001\n",
      "output index is :  22\n",
      "correct index is :  0\n",
      "corrent rs is :  0.16\n",
      "train mean loss is :  1.4559256276488304\n",
      "valid mean loss is :  1.4619300825595856\n",
      "train mean reciprocal score is :  0.5693572965720887\n",
      "valid mean reciprocal score is :  0.5648812602096304\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 4001\n",
      "output index is :  9\n",
      "correct index is :  6\n",
      "corrent rs is :  0.18181818181818182\n",
      "train mean loss is :  1.4500816786289215\n",
      "valid mean loss is :  1.4594952515363693\n",
      "train mean reciprocal score is :  0.5762051264316521\n",
      "valid mean reciprocal score is :  0.5669275441628487\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 5001\n",
      "output index is :  0\n",
      "correct index is :  15\n",
      "corrent rs is :  0.96\n",
      "train mean loss is :  1.4434542665481567\n",
      "valid mean loss is :  1.4559882400035857\n",
      "train mean reciprocal score is :  0.580754356677258\n",
      "valid mean reciprocal score is :  0.5707808052532858\n",
      "data num is :  1000\n",
      "step 6001\n",
      "output index is :  21\n",
      "correct index is :  7\n",
      "corrent rs is :  0.30434782608695654\n",
      "train mean loss is :  1.4276060409545899\n",
      "valid mean loss is :  1.4565730611085892\n",
      "train mean reciprocal score is :  0.5984836427629516\n",
      "valid mean reciprocal score is :  0.5699403788823091\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 7001\n",
      "output index is :  2\n",
      "correct index is :  0\n",
      "corrent rs is :  0.2\n",
      "train mean loss is :  1.430706982076168\n",
      "valid mean loss is :  1.4548186098337172\n",
      "train mean reciprocal score is :  0.5953935269682016\n",
      "valid mean reciprocal score is :  0.5718729037035493\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 8001\n",
      "output index is :  9\n",
      "correct index is :  11\n",
      "corrent rs is :  0.44\n",
      "train mean loss is :  1.4217386966943741\n",
      "valid mean loss is :  1.442378184556961\n",
      "train mean reciprocal score is :  0.6050194493139404\n",
      "valid mean reciprocal score is :  0.5845130548175594\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 9001\n",
      "output index is :  10\n",
      "correct index is :  6\n",
      "corrent rs is :  0.6190476190476191\n",
      "train mean loss is :  1.4270692709088326\n",
      "valid mean loss is :  1.4407223917245864\n",
      "train mean reciprocal score is :  0.5994911297610047\n",
      "valid mean reciprocal score is :  0.5860211604962252\n",
      "data num is :  1000\n",
      "step 10001\n",
      "output index is :  22\n",
      "correct index is :  0\n",
      "corrent rs is :  0.96\n",
      "train mean loss is :  1.417552107810974\n",
      "valid mean loss is :  1.4442342494726181\n",
      "train mean reciprocal score is :  0.6088487152681443\n",
      "valid mean reciprocal score is :  0.5827037131450022\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 11001\n",
      "output index is :  15\n",
      "correct index is :  5\n",
      "corrent rs is :  0.84\n",
      "train mean loss is :  1.4276253443956375\n",
      "valid mean loss is :  1.431031200528145\n",
      "train mean reciprocal score is :  0.5980012870759842\n",
      "valid mean reciprocal score is :  0.5953365549336398\n",
      "data num is :  1000\n",
      "step 12001\n",
      "output index is :  0\n",
      "correct index is :  24\n",
      "corrent rs is :  0.68\n",
      "train mean loss is :  1.418468741953373\n",
      "valid mean loss is :  1.4321118748784065\n",
      "train mean reciprocal score is :  0.605831909359653\n",
      "valid mean reciprocal score is :  0.5945989499820555\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 13001\n",
      "output index is :  1\n",
      "correct index is :  0\n",
      "corrent rs is :  0.3333333333333333\n",
      "train mean loss is :  1.3944248864650726\n",
      "valid mean loss is :  1.413482568860054\n",
      "train mean reciprocal score is :  0.6296010998052434\n",
      "valid mean reciprocal score is :  0.6139850115894927\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 14001\n",
      "output index is :  11\n",
      "correct index is :  0\n",
      "corrent rs is :  0.08\n",
      "train mean loss is :  1.4150596589446067\n",
      "valid mean loss is :  1.408351883649826\n",
      "train mean reciprocal score is :  0.6100460165891155\n",
      "valid mean reciprocal score is :  0.6185198606845516\n",
      "data num is :  1000\n",
      "step 15001\n",
      "output index is :  2\n",
      "correct index is :  0\n",
      "corrent rs is :  0.72\n",
      "train mean loss is :  1.413386190533638\n",
      "valid mean loss is :  1.4312445044517517\n",
      "train mean reciprocal score is :  0.6145107259375139\n",
      "valid mean reciprocal score is :  0.595250601554448\n",
      "data num is :  1000\n",
      "step 16001\n",
      "output index is :  21\n",
      "correct index is :  1\n",
      "corrent rs is :  0.2\n",
      "train mean loss is :  1.4371491679549218\n",
      "valid mean loss is :  1.445094683289528\n",
      "train mean reciprocal score is :  0.5866501126401239\n",
      "valid mean reciprocal score is :  0.5816163458503102\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 17001\n",
      "output index is :  3\n",
      "correct index is :  21\n",
      "corrent rs is :  0.68\n",
      "train mean loss is :  1.4204332094192504\n",
      "valid mean loss is :  1.4050085555315017\n",
      "train mean reciprocal score is :  0.6054716037930385\n",
      "valid mean reciprocal score is :  0.6213944368813006\n",
      "data num is :  1000\n",
      "step 18001\n",
      "output index is :  23\n",
      "correct index is :  16\n",
      "corrent rs is :  0.76\n",
      "train mean loss is :  1.3932781716585159\n",
      "valid mean loss is :  1.4125887882709502\n",
      "train mean reciprocal score is :  0.6335419670374274\n",
      "valid mean reciprocal score is :  0.6146842157915368\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 19001\n",
      "output index is :  0\n",
      "correct index is :  0\n",
      "corrent rs is :  1.0\n",
      "train mean loss is :  1.3864786949753762\n",
      "valid mean loss is :  1.3953663252592088\n",
      "train mean reciprocal score is :  0.6380517214139708\n",
      "valid mean reciprocal score is :  0.6310696119629458\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 20001\n",
      "output index is :  23\n",
      "correct index is :  0\n",
      "corrent rs is :  0.875\n",
      "train mean loss is :  1.390262515425682\n",
      "valid mean loss is :  1.3914229792356492\n",
      "train mean reciprocal score is :  0.6383680593338389\n",
      "valid mean reciprocal score is :  0.6348237579095053\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 21001\n",
      "output index is :  22\n",
      "correct index is :  3\n",
      "corrent rs is :  0.68\n",
      "train mean loss is :  1.4050895159244536\n",
      "valid mean loss is :  1.385001593708992\n",
      "train mean reciprocal score is :  0.6208001595089819\n",
      "valid mean reciprocal score is :  0.6404491689632438\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 22001\n",
      "output index is :  5\n",
      "correct index is :  7\n",
      "corrent rs is :  0.4\n",
      "train mean loss is :  1.3853944422006608\n",
      "valid mean loss is :  1.3828807296156884\n",
      "train mean reciprocal score is :  0.6390378283697878\n",
      "valid mean reciprocal score is :  0.6439094827535654\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 23001\n",
      "output index is :  13\n",
      "correct index is :  5\n",
      "corrent rs is :  0.88\n",
      "train mean loss is :  1.39440007686615\n",
      "valid mean loss is :  1.3769101949334144\n",
      "train mean reciprocal score is :  0.6311555789832434\n",
      "valid mean reciprocal score is :  0.6499536213549013\n",
      "data num is :  1000\n",
      "step 24001\n",
      "output index is :  7\n",
      "correct index is :  7\n",
      "corrent rs is :  1.0\n",
      "train mean loss is :  1.3773011659383774\n",
      "valid mean loss is :  1.3774599856734275\n",
      "train mean reciprocal score is :  0.6492050455514332\n",
      "valid mean reciprocal score is :  0.648767729455263\n",
      "data num is :  1000\n",
      "step 25001\n",
      "output index is :  1\n",
      "correct index is :  0\n",
      "corrent rs is :  0.375\n",
      "train mean loss is :  1.4052007174491883\n",
      "valid mean loss is :  1.3793263713121413\n",
      "train mean reciprocal score is :  0.6210176330284184\n",
      "valid mean reciprocal score is :  0.6475258914851983\n",
      "data num is :  1000\n",
      "session saved\n",
      "step 26001\n",
      "output index is :  0\n",
      "correct index is :  0\n",
      "corrent rs is :  1.0\n",
      "train mean loss is :  1.3826426045298577\n",
      "valid mean loss is :  1.3747414246797562\n",
      "train mean reciprocal score is :  0.6422097211168905\n",
      "valid mean reciprocal score is :  0.6515930908171039\n",
      "data num is :  1000\n"
     ]
    }
   ],
   "source": [
    "final_train_data = train_data_preprocess[:810000]\n",
    "final_valid_data = train_data_preprocess[810000:811000]\n",
    "final_test_data = train_data_preprocess[811000:]\n",
    "num_train = len(final_train_data)\n",
    "num_valid = len(final_valid_data)\n",
    "num_test = len(final_test_data)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "max_mrs = 0\n",
    "#all_data_number = len(train_data_preprocess)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    mean_loss = 0\n",
    "    mean_rs = 0\n",
    "    data_number = 0\n",
    "    for i in range(num_train):\n",
    "        #print(i)\n",
    "        #if i>100:\n",
    "        #    break\n",
    "        \n",
    "        \n",
    "        batch_data, batch_items, correct_index = MakeEmbeddingVector3(final_train_data[i])\n",
    "        batch_data  = np.asarray(batch_data)\n",
    "        batch_items = np.asarray(batch_items)\n",
    "\n",
    "        sess.run(optimizer, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "        ls = sess.run(loss2, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "        _score = sess.run(scores, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "        if(len(_score) == 1):\n",
    "            current_rs = 1\n",
    "        else:\n",
    "            current_rs = get_MRS(_score, correct_index[0])\n",
    "\n",
    "            \n",
    "        #print(correct_index)\n",
    "        #print(_score)\n",
    "        #print(np.argmax(_score, axis = 0)[0])\n",
    "        #print(correct_index[0])\n",
    "\n",
    "        mean_rs += current_rs \n",
    "        mean_loss += ls\n",
    "        data_number +=1\n",
    "        if (step % 1000 == 1):\n",
    "            valid_mean_rs = 0\n",
    "            valid_mean_loss = 0\n",
    "            for j in range(num_valid):\n",
    "                valid_batch_data, valid_batch_items, valid_correct_index = MakeEmbeddingVector3(final_valid_data[j])\n",
    "                valid_batch_data  = np.asarray(valid_batch_data)\n",
    "                valid_batch_items = np.asarray(valid_batch_items)\n",
    "                valid_loss = sess.run(loss2, feed_dict = {train_data : valid_batch_data, train_items : valid_batch_items, train_index : valid_correct_index})\n",
    "                valid_score = sess.run(scores, feed_dict = {train_data : valid_batch_data, train_items : valid_batch_items, train_index : valid_correct_index})\n",
    "                if(len(valid_score) == 1):\n",
    "                    valid_current_rs = 1\n",
    "                else:\n",
    "                    valid_current_rs = get_MRS(valid_score, valid_correct_index[0])\n",
    "                valid_mean_rs += valid_current_rs\n",
    "                valid_mean_loss += valid_loss\n",
    "            if(valid_mean_rs / num_valid > max_mrs):\n",
    "                max_mrs = valid_mean_rs /num_valid\n",
    "                print('session saved')\n",
    "                saver.save(sess, './saved_model/task1_ver2')\n",
    "            print('step', step )\n",
    "            print(\"output index is : \", np.argmax(_score, axis = 0)[0])\n",
    "            print(\"correct index is : \", correct_index[0])\n",
    "            print(\"corrent rs is : \", current_rs)\n",
    "            \n",
    "            print(\"train mean loss is : \", mean_loss / data_number)\n",
    "            print(\"valid mean loss is : \", valid_mean_loss / num_valid)\n",
    "            \n",
    "            print(\"train mean reciprocal score is : \", mean_rs / data_number)\n",
    "            print(\"valid mean reciprocal score is : \", valid_mean_rs / num_valid)\n",
    "            \n",
    "            print(\"data num is : \", data_number)\n",
    "            '''\n",
    "            if(mean_rs / data_number > max_mrs):\n",
    "                max_mrs = mean_rs /data_number\n",
    "                print('saved')\n",
    "                saver.save(sess, './saved_model/task1_ver2')\n",
    "            '''\n",
    "            mean_rs =0\n",
    "            mean_loss = 0\n",
    "            data_number = 0\n",
    "            \n",
    "        step +=1\n",
    "                \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        mean_loss = 0\n",
    "        mean_rs = 0\n",
    "        saver.restore(sess, './saved_model/task1')\n",
    "        num_valid = len(final_valid_data)\n",
    "        for i in range(num_valid):\n",
    "\n",
    "                batch_data, batch_items, correct_index = MakeEmbeddingVector3(final_valid_data[i])\n",
    "                batch_data  = np.asarray(batch_data)\n",
    "                batch_items = np.asarray(batch_items)\n",
    "\n",
    "                #sess.run(optimizer, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "                ls = sess.run(loss2, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "                _score = sess.run(scores, feed_dict = {train_data : batch_data, train_items : batch_items, train_index : correct_index})\n",
    "                if(len(_score) == 1):\n",
    "                    current_rs = 1\n",
    "                else:\n",
    "                    current_rs = get_MRS(_score, correct_index[0])\n",
    "\n",
    "                mean_rs += current_rs \n",
    "                mean_loss += ls\n",
    "                \n",
    "        print(\"mean loss is : \", mean_loss /num_valid)\n",
    "        print(\"mean reciprocal score is : \", mean_rs /num_valid)\n",
    "        return mean_loss /num_valid, mean_rs /num_valid            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model/task1\n",
      "mean loss is :  1.3136600736365127\n",
      "mean reciprocal score is :  0.7130262906034744\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mear_rs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-009689b947a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-171-4437e69da80e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean loss is : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_loss\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mnum_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reciprocal score is : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_rs\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mnum_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmean_loss\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mnum_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmear_rs\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0mnum_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mear_rs' is not defined"
     ]
    }
   ],
   "source": [
    "a, b = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
